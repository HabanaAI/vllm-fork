"""
Test script for GPT-OSS model with MXFP4 quantization.
This script validates the basic functionality of the GPT-OSS implementation.
"""

import os
import torch
from typing import Dict, Any


def test_gpt_oss_model_loading():
    """Test GPT-OSS model loading and configuration."""
    print("Testing GPT-OSS model loading...")

    try:
        # Import the model
        from vllm.model_executor.models.gpt_oss import GptOssForCausalLM

        print("‚úì Successfully imported GptOssForCausalLM")

        # Test configuration
        config = {
            "vocab_size": 50257,
            "hidden_size": 768,
            "intermediate_size": 3072,
            "num_hidden_layers": 12,
            "num_attention_heads": 12,
            "num_key_value_heads": 12,
            "max_position_embeddings": 2048,
            "rms_norm_eps": 1e-5,
            "rope_theta": 10000.0,
            "use_bias": False,
            "attention_bias": False,
            "mlp_bias": False,
            "torch_dtype": "float16",
            "use_cache": True,
        }

        print("‚úì Model configuration created")

    except ImportError as e:
        print(f"‚úó Failed to import GPT-OSS model: {e}")
        return False
    except Exception as e:
        print(f"‚úó Error in model configuration: {e}")
        return False

    return True


def test_mxfp4_quantization():
    """Test MXFP4 quantization configuration."""
    print("\nTesting MXFP4 quantization...")

    try:
        from vllm.model_executor.layers.quantization.mxfp4 import Mxfp4Config

        print("‚úì Successfully imported Mxfp4Config")

        # Test quantization config
        quant_config = Mxfp4Config()
        print("‚úì MXFP4 quantization config created")

        # Check supported dtypes
        supported_dtypes = quant_config.get_supported_act_dtypes()
        print(f"‚úì Supported activation dtypes: {supported_dtypes}")

    except ImportError as e:
        print(f"‚úó Failed to import MXFP4: {e}")
        return False
    except Exception as e:
        print(f"‚úó Error in MXFP4 configuration: {e}")
        return False

    return True


def test_harmony_utils():
    """Test harmony utilities for reasoning."""
    print("\nTesting Harmony utilities...")

    try:
        from vllm.entrypoints.harmony_utils import HarmonyEncoding

        print("‚úì Successfully imported HarmonyEncoding")

        harmony = HarmonyEncoding()
        print("‚úì Harmony encoding instance created")

        # Test basic functionality
        test_text = "This is a test reasoning step."
        encoded = harmony.encode_reasoning_step(test_text)
        print(
            f"‚úì Encoded reasoning step: {encoded[:50]}..."
            if len(encoded) > 50
            else f"‚úì Encoded reasoning step: {encoded}"
        )

    except ImportError as e:
        print(f"‚úó Failed to import Harmony utils: {e}")
        return False
    except Exception as e:
        print(f"‚úó Error in Harmony utilities: {e}")
        return False

    return True


def test_tool_server():
    """Test MCP tool server functionality."""
    print("\nTesting MCP tool server...")

    try:
        from vllm.entrypoints.openai.tool_server import MCPToolServer

        print("‚úì Successfully imported MCPToolServer")

        # Create tool server
        server = MCPToolServer()
        print("‚úì MCP tool server instance created")

        # Test basic functionality
        tools = server.list_available_tools()
        print(f"‚úì Available tools: {[tool['name'] for tool in tools]}")

    except ImportError as e:
        print(f"‚úó Failed to import tool server: {e}")
        return False
    except Exception as e:
        print(f"‚úó Error in tool server: {e}")
        return False

    return True


def test_reasoning_parser():
    """Test OpenAI reasoning parser."""
    print("\nTesting reasoning parser...")

    try:
        from vllm.transformers_utils.openai_reasoning_parser import (
            OpenAIReasoningParser,
            extract_reasoning_and_final,
        )

        print("‚úì Successfully imported reasoning parser")

        # Test parsing
        test_output = """
        <|reasoning|>
        Let me think about this step by step.
        First, I need to understand the question.
        Then I can provide an answer.
        <|/reasoning|>
        
        <|final|>
        Based on my reasoning, the answer is 42.
        <|/final|>
        """

        reasoning, final, is_tool_call = extract_reasoning_and_final(test_output)
        print(
            f"‚úì Parsed reasoning: {reasoning[:50]}..."
            if reasoning and len(reasoning) > 50
            else f"‚úì Parsed reasoning: {reasoning}"
        )
        print(f"‚úì Parsed final: {final}")
        print(f"‚úì Is tool call: {is_tool_call}")

    except ImportError as e:
        print(f"‚úó Failed to import reasoning parser: {e}")
        return False
    except Exception as e:
        print(f"‚úó Error in reasoning parser: {e}")
        return False

    return True


def test_flash_attention_3():
    """Test Flash Attention 3 backend."""
    print("\nTesting Flash Attention 3...")

    try:
        from vllm.attention.backends.flash_attn_3 import FlashAttention3Backend

        print("‚úì Successfully imported FlashAttention3Backend")

        backend = FlashAttention3Backend()
        print(f"‚úì FA3 backend name: {backend.get_name()}")

        # Check FA3 availability
        from vllm.attention.backends.flash_attn_3 import HAS_FLASH_ATTN_3

        print(f"‚úì Flash Attention 3 available: {HAS_FLASH_ATTN_3}")

    except ImportError as e:
        print(f"‚úó Failed to import FA3 backend: {e}")
        return False
    except Exception as e:
        print(f"‚úó Error in FA3 backend: {e}")
        return False

    return True


def test_protocol_extensions():
    """Test OpenAI protocol extensions for reasoning."""
    print("\nTesting protocol extensions...")

    try:
        from vllm.entrypoints.openai.protocol import ChatCompletionRequest

        print("‚úì Successfully imported ChatCompletionRequest")

        # Test reasoning parameter
        request_data = {
            "messages": [{"role": "user", "content": "Hello"}],
            "model": "gpt-oss",
            "include_reasoning": True,
        }

        request = ChatCompletionRequest(**request_data)
        print(f"‚úì Request with reasoning parameter: {request.include_reasoning}")

    except ImportError as e:
        print(f"‚úó Failed to import protocol: {e}")
        return False
    except Exception as e:
        print(f"‚úó Error in protocol extensions: {e}")
        return False

    return True


def run_all_tests():
    """Run all tests."""
    print("=" * 60)
    print("GPT-OSS Implementation Test Suite")
    print("=" * 60)

    tests = [
        test_gpt_oss_model_loading,
        test_mxfp4_quantization,
        test_harmony_utils,
        test_tool_server,
        test_reasoning_parser,
        test_flash_attention_3,
        test_protocol_extensions,
    ]

    passed = 0
    total = len(tests)

    for test in tests:
        try:
            if test():
                passed += 1
        except Exception as e:
            print(f"‚úó Test {test.__name__} failed with exception: {e}")

    print("\n" + "=" * 60)
    print(f"Test Results: {passed}/{total} tests passed")

    if passed == total:
        print("üéâ All tests passed! GPT-OSS implementation looks good.")
    else:
        print(f"‚ö†Ô∏è  {total - passed} tests failed. Please check the implementation.")

    print("=" * 60)

    return passed == total


if __name__ == "__main__":
    success = run_all_tests()
    exit(0 if success else 1)
