diff --git a/dynamic_quant.json b/dynamic_quant.json
new file mode 100644
index 000000000..8eebbb77c
--- /dev/null
+++ b/dynamic_quant.json
@@ -0,0 +1,9 @@
+{
+
+    "mode": "QUANTIZE",
+    "scale_method": "MAXABS_POW2_DYNAMIC",
+    "scale_format": "CONST",
+    "dump_stats_path": "./hqt_output/measure"
+
+}
+
diff --git a/scripts/run_lm_eval.py b/scripts/run_lm_eval.py
index 4bd9b15aa..79e8e870b 100644
--- a/scripts/run_lm_eval.py
+++ b/scripts/run_lm_eval.py
@@ -15,22 +15,33 @@ parser.add_argument("--task", type=str, default="gsm8k", help="The model path.")
 parser.add_argument("--tokenizer", type=str, default=model_path, help="The model path.")
 parser.add_argument("--tp_size", type=int, default=8, help="Tensor Parallelism size.")
 parser.add_argument("--ep_size", type=int, default=4, help="Expert Parallelism size.")
+parser.add_argument("--inc_quant", action="store_true", help="Enable inc_quant mode.")
 parser.add_argument("-l", "--limit", type=int, default=16, help="test request counts.")
 args = parser.parse_args()

+#os.environ["VLLM_SKIP_WARMUP"] = "true"
+#os.environ["HABANA_VISIBLE_DEVICES"] = "ALL"
+#os.environ["PT_HPU_ENABLE_LAZY_COLLECTIVES"] = "true"
+#if args.ep_size > 1:
+#    os.environ["VLLM_MOE_N_SLICE"] = "1"
+#    os.environ["VLLM_EP_SIZE"] = f"{args.ep_size}"
+#else:
+#    os.environ["VLLM_MOE_N_SLICE"] = "4"
+#    os.environ["VLLM_EP_SIZE"] = "1"
+
+#os.environ["VLLM_MLA_DISABLE_REQUANTIZATION"] = "1"
+#os.environ["PT_HPU_WEIGHT_SHARING"] = "0"
 os.environ["VLLM_SKIP_WARMUP"] = "true"
 os.environ["HABANA_VISIBLE_DEVICES"] = "ALL"
 os.environ["PT_HPU_ENABLE_LAZY_COLLECTIVES"] = "true"
-if args.ep_size > 1:
-    os.environ["VLLM_MOE_N_SLICE"] = "1"
-    os.environ["VLLM_EP_SIZE"] = f"{args.ep_size}"
-else:
-    os.environ["VLLM_MOE_N_SLICE"] = "4"
-    os.environ["VLLM_EP_SIZE"] = "1"
-
+# os.environ["VLLM_RAY_DISABLE_LOG_TO_DRIVER"] = "1"
+# os.environ["RAY_IGNORE_UNHANDLED_ERRORS"] = "1"
+os.environ["VLLM_MOE_N_SLICE"] = "1"
+os.environ["VLLM_EP_SIZE"] = "8"
 os.environ["VLLM_MLA_DISABLE_REQUANTIZATION"] = "1"
 os.environ["PT_HPU_WEIGHT_SHARING"] = "0"

+
 if __name__ == "__main__":

     from lm_eval.models.vllm_causallms import VLLM
@@ -46,9 +57,22 @@ if __name__ == "__main__":
             max_model_len=4096,
             gpu_memory_utilization=0.8,
         )
+    elif args.inc_quant:
+        llm = VLLM(
+            pretrained=model,
+            quantization="inc",
+            weights_load_device="cpu",
+            tokenizer=args.tokenizer,
+            tensor_parallel_size=args.tp_size,
+            distributed_executor_backend='mp',
+            trust_remote_code=True,
+            max_model_len=4096,
+            dtype="bfloat16",
+            gpu_memory_utilization=0.8,
+        )
     else:
         llm = VLLM(
-            pretrained=model,
+            pretrained=model,
             tokenizer=args.tokenizer,
             tensor_parallel_size=args.tp_size,
             distributed_executor_backend='mp',
@@ -84,4 +108,4 @@ if __name__ == "__main__":
     # Print out the results.
     print("Evaluation Results:")
     for task, metrics in results['results'].items():
-        print(f"{task}: {metrics}")
\ No newline at end of file
+        print(f"{task}: {metrics}")
diff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py
index 8b136de33..13b41dea4 100644
--- a/vllm/model_executor/layers/quantization/fp8.py
+++ b/vllm/model_executor/layers/quantization/fp8.py
@@ -32,7 +32,10 @@ from vllm.platforms import current_platform

 if current_platform.is_hpu():
     from vllm_hpu_extension.ops import scaled_fp8_quant
+    #import habana_frameworks.torch.core as htcore
+    #import habana_frameworks.torch.utils.experimental as htexp
     ops.scaled_fp8_quant = scaled_fp8_quant
+    #cast_from_fp8_fcn = lambda x, dtype, scale=None: torch.ops.hpu.cast_from_fp8(x, scale, dtype)

 ACTIVATION_SCHEMES = ["static", "dynamic"]

@@ -779,39 +782,47 @@ class Fp8MoEMethod(FusedMoEMethodBase):
             scoring_func=scoring_func,
             e_score_correction_bias=e_score_correction_bias)
         final_hidden_states = torch.zeros_like(x)
-        num_experts = layer.w13_weight.shape[0]
-        n_expert_slice = layer.w13_weight.shape[0] // self.moe_n_slice
-        assert n_expert_slice * self.moe_n_slice == num_experts
-
-        # w13_list = layer.hpu_fused_moe.MoeOp.w13_list
-        # w2_list = layer.hpu_fused_moe.MoeOp.w2_list
-        from vllm.model_executor.layers.quantization.utils.fp8_utils import dequant_block_fp8_weight_naive
-
-        orig_M_w13 = layer.orig_M_w13.data
-        orig_N_w13 = layer.orig_N_w13.data
-        orig_M_w2 = layer.orig_M_w2.data
-        orig_N_w2 = layer.orig_N_w2.data
+        #num_experts = layer.w13_weight.shape[0]
+        #n_expert_slice = layer.w13_weight.shape[0] // self.moe_n_slice
+        #assert n_expert_slice * self.moe_n_slice == num_experts
+        num_experts = layer.num_experts
+        n_expert_slice = num_experts // self.moe_n_slice
+
+        w13_list = layer.hpu_fused_moe.MoeOp.w13_list
+        w2_list = layer.hpu_fused_moe.MoeOp.w2_list
+        #from vllm.model_executor.layers.quantization.utils.fp8_utils import dequant_block_fp8_weight_naive
+
+        #orig_M_w13 = layer.orig_M_w13.data
+        #orig_N_w13 = layer.orig_N_w13.data
+        #orig_M_w2 = layer.orig_M_w2.data
+        #orig_N_w2 = layer.orig_N_w2.data
         ep_shift = ep_rank * num_experts
-        w13_weight = dequant_block_fp8_weight_naive(layer.w13_weight,
-                                                    layer.w13_weight_scale_inv,
-                                                    block_size=self.quant_config.weight_block_size,
-                                                    dtype=x.dtype,
-                                                    original_M=orig_M_w13,
-                                                    original_N=orig_N_w13)
-        w2_weight = dequant_block_fp8_weight_naive(layer.w2_weight,
-                                                    layer.w2_weight_scale_inv,
-                                                    block_size=self.quant_config.weight_block_size,
-                                                    dtype=x.dtype,
-                                                    original_M=orig_M_w2,
-                                                    original_N=orig_N_w2)
+        #w13_weight = dequant_block_fp8_weight_naive(layer.w13_weight,
+        #                                            layer.w13_weight_scale_inv,
+        #                                            block_size=self.quant_config.weight_block_size,
+        #                                            dtype=x.dtype,
+        #                                            original_M=orig_M_w13,
+        #                                            original_N=orig_N_w13)
+        #w2_weight = dequant_block_fp8_weight_naive(layer.w2_weight,
+        #                                            layer.w2_weight_scale_inv,
+        #                                            block_size=self.quant_config.weight_block_size,
+        #                                            dtype=x.dtype,
+        #                                            original_M=orig_M_w2,
+        #                                            original_N=orig_N_w2)
         for i in range(self.moe_n_slice):
             min_expert = i * n_expert_slice
             max_expert = (i + 1) * n_expert_slice

-            w13_list_slice = [w13_weight[j] for j in range(min_expert, max_expert)]
-            w2_list_slice = [w2_weight[j] for j in range(min_expert, max_expert)]
+            #w13_list_slice = [w13_weight[j] for j in range(min_expert, max_expert)]
+            #w2_list_slice = [w2_weight[j] for j in range(min_expert, max_expert)]
+            w13_list_slice = [w13_list[j].weight.t().data for j in range(min_expert, max_expert)]
+            
+            #w13_list_slice = [cast_from_fp8_fcn(w13_list[j].weight.t().data, torch.bfloat16, w13_list[j].scale_weight) for j in range(min_expert, max_expert)]
+            #w2_list_slice = [cast_from_fp8_fcn(w2_list[j].weight.t().data, torch.bfloat16, w13_list[j].scale_weight) for j in range(min_expert, max_expert)]

-            final_hidden_states += torch.ops.hpu.mixture_of_experts(
+            #final_hidden_states += torch.ops.hpu.mixture_of_experts(
+            final_hidden_states += torch.ops.hpu.mixture_of_experts.(
                                          hidden_states=x,
                                          expert_routing_table=topk_ids.to(torch.int64),
                                          router_weights=topk_weights.to(x.dtype),

