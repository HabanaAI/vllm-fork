# FIXME(kzawora): these scores were generated using vLLM on HPU, we need to confirm them on HF
# VLLM_SKIP_WARMUP=true bash run-lm-eval-gsm-cot-llama-vllm-baseline.sh -m "/mnt/weka/data/pytorch/llama3.1/Meta-Llama-3.1-8B-Instruct" -b 128 -l 1319 -f 8 -t 1
model_name: "/root/software/data/pytorch/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/"
tasks:
- name: "gsm8k_cot_llama"
  metrics:
  - name: "exact_match,strict-match"
    value: 0.788
  - name: "exact_match,flexible-extract"
    value: 0.788
limit: 250
num_fewshot: 8
dtype: "bfloat16"
fewshot_as_multiturn: true
apply_chat_template: true
