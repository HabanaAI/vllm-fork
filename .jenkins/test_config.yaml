# test_config.yaml
steps:
  - name: lm_eval_gsm8k_llama3_1_8b_g1
    flavor: g1
    command: lm_eval --model vllm --model_args pretrained=/mnt/weka/data/pytorch/llama3.1/Meta-Llama-3.1-8B-Instruct/,dtype=bfloat16,max_num_seqs=128 --tasks gsm8k_cot_llama --fewshot_as_multiturn --apply_chat_template --batch_size auto
  - name: lm_eval_gsm8k_llama3_1_8b_g2
    flavor: g2
    command: lm_eval --model vllm --model_args pretrained=/mnt/weka/data/pytorch/llama3.1/Meta-Llama-3.1-8B-Instruct/,dtype=bfloat16,max_num_seqs=128 --tasks gsm8k_cot_llama --fewshot_as_multiturn --apply_chat_template --batch_size auto
  - name: lm_eval_gsm8k_llama3_1_8b_g3
    flavor: g3
    command: lm_eval --model vllm --model_args pretrained=/mnt/weka/data/pytorch/llama3.1/Meta-Llama-3.1-8B-Instruct/,dtype=bfloat16,max_num_seqs=128 --tasks gsm8k_cot_llama --fewshot_as_multiturn --apply_chat_template --batch_size auto
