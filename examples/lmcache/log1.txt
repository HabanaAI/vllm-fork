INFO 05-01 22:47:41 [__init__.py:248] Automatically detected platform hpu.
INFO 05-01 22:47:49 [config.py:602] This model supports multiple tasks: {'reward', 'score', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
WARNING 05-01 22:47:49 [arg_utils.py:1775] Detected VLLM_USE_V1=1 with hpu. Usage should be considered experimental. Please report any issues on Github.
INFO 05-01 22:47:49 [config.py:1643] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 05-01 22:47:49 [config.py:1799] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 05-01 22:47:50 [core.py:61] Initializing a V1 LLM engine (v0.6.3.dev3761+g6d938e09d.d20250430) with config: model='/root/mnt/weka/data/pytorch/llama3.1/Meta-Llama-3.1-8B-Instruct/', speculative_config=None, tokenizer='/root/mnt/weka/data/pytorch/llama3.1/Meta-Llama-3.1-8B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=hpu, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/root/mnt/weka/data/pytorch/llama3.1/Meta-Llama-3.1-8B-Instruct/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
INFO 05-01 22:47:50 [parallel_state.py:943] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
Creating v1 connector with name: %s LMCacheConnectorV1
WARNING 05-01 22:47:53 [base.py:58] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
INFO 05-01 22:47:56 [hpu_model_runner.py:178] HpuEnvFlags VLLM_SKIP_WARMUP: True
INFO 05-01 22:47:56 [hpu_model_runner.py:178] HpuEnvFlags VLLM_ENABLE_BUCKETING: True
INFO 05-01 22:47:56 [hpu_model_runner.py:178] HpuEnvFlags VLLM_CONTIGUOUS_PA: False
WARNING 05-01 22:47:56 [hpu.py:99] Pin memory is not supported on HPU.
INFO 05-01 22:47:56 [hpu.py:39] Using HPUAttentionV1 backend.
INFO 05-01 22:47:56 [hpu_model_runner.py:681] Bucketing is ON.
INFO 05-01 22:47:56 [hpu_model_runner.py:1811] Starting to load model /root/mnt/weka/data/pytorch/llama3.1/Meta-Llama-3.1-8B-Instruct/...
INFO 05-01 22:47:56 [hpu.py:39] Using HPUAttentionV1 backend.
Detected flags: [-compile_one_hot -cpu -flex_attention -fp32_softmax +fsdpa -gaudi +gaudi2 -gaudi3]
INFO 05-01 22:47:57 [loader.py:447] Loading weights on hpu...
INFO 05-01 22:48:00 [core_client.py:416] Waiting for 1 core engine proc(s) to start: {0}
INFO 05-01 22:48:00 [loader.py:452] Loading weights took 3.23 seconds
INFO 05-01 22:48:00 [hpu_model_runner.py:1815] Loading model weights took 15.1179 GB
INFO 05-01 22:48:01 [hpu_model_runner.py:1832] Wrapping in HPUGraph took 0.0000 GB
INFO 05-01 22:48:01 [hpu_worker.py:165] Model profiling run took 0 B of device memory (15.12 GiB/94.62 GiB used) and 0 B of host memory (321.8 GiB/1007 GiB used)
INFO 05-01 22:48:01 [hpu_worker.py:192] Free device memory: 79.51 GiB, 63.61 GiB usable (gpu_memory_utilization=0.8), 6.361 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 16 MiB reserved for KV cache dummy block 57.23 GiB reserved for usable KV cache
INFO 05-01 22:48:01 [kv_cache_utils.py:578] GPU KV cache size: 468,736 tokens
INFO 05-01 22:48:01 [kv_cache_utils.py:581] Maximum concurrency for 8,000 tokens per request: 58.59x
INFO 05-01 22:48:01 [hpu_model_runner.py:2051] Skipping warmup...
INFO 05-01 22:48:01 [core.py:159] init engine (profile, create kv cache, warmup model) took 0.44 seconds
Creating v1 connector with name: %s LMCacheConnectorV1
WARNING 05-01 22:48:01 [base.py:58] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
INFO 05-01 22:48:01 [core_client.py:424] Core engine process 0 ready.
WARNING 05-01 22:48:01 [hpu_model_runner.py:1462] Configuration: (prefill, 1, 6016, 0) was not warmed-up!
libin debug from_gpu, copy start/end/shape  0 256
libin debug from_gpu, copy start/end/shape  256 512
libin debug from_gpu, copy start/end/shape  512 768
libin debug from_gpu, copy start/end/shape  768 1024
libin debug from_gpu, copy start/end/shape  1024 1280
libin debug from_gpu, copy start/end/shape  1280 1536
libin debug from_gpu, copy start/end/shape  1536 1792
libin debug from_gpu, copy start/end/shape  1792 2048
libin debug from_gpu, copy start/end/shape  2048 2304
libin debug from_gpu, copy start/end/shape  2304 2560
libin debug from_gpu, copy start/end/shape  2560 2816
libin debug from_gpu, copy start/end/shape  2816 3072
libin debug from_gpu, copy start/end/shape  3072 3328
libin debug from_gpu, copy start/end/shape  3328 3584
libin debug from_gpu, copy start/end/shape  3584 3840
libin debug from_gpu, copy start/end/shape  3840 4096
libin debug from_gpu, copy start/end/shape  4096 4352
libin debug from_gpu, copy start/end/shape  4352 4608
libin debug from_gpu, copy start/end/shape  4608 4864
libin debug from_gpu, copy start/end/shape  4864 5120
libin debug from_gpu, copy start/end/shape  5120 5376
libin debug from_gpu, copy start/end/shape  5376 5632
libin debug from_gpu, copy start/end/shape  5632 5888
libin debug from_gpu, copy start/end/shape  5888 6006
WARNING 05-01 22:48:03 [hpu_model_runner.py:1462] Configuration: (decode, 1, 1, 128) was not warmed-up!
Generated text: ' John. I am a 25-year-old man'
