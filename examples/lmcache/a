17a18
> from lmcache.experimental.cache_engine import LMCacheEngineBuilder
19d19
< from lmcache.v1.cache_engine import LMCacheEngineBuilder
26a27,28
> # Use experimental features in LMCache
> os.environ["LMCACHE_USE_EXPERIMENTAL"] = "True"
39,42d40
< MODEL = "/software/data/pytorch/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/"
< #prompts = [
< #    "Hello, how are you?" * 1000,
< #]
44c42
<     "San Francisco is a",
---
>     "Hello, how are you?" * 1000,
49a48,49
>     os.environ["CUDA_VISIBLE_DEVICES"] = "0"
> 
53c53
<         '{"kv_connector":"LMCacheConnectorV1", "kv_role":"kv_producer"}')
---
>         '{"kv_connector":"LMCacheConnectorV1", "kv_role":"kv_both"}')
56c56
<     llm = LLM(model=MODEL,
---
>     llm = LLM(model="mistralai/Mistral-7B-Instruct-v0.2",
60,61c60
<               tensor_parallel_size=2,
<               enforce_eager=False)
---
>               enforce_eager=True)
66c65
<         print(f"Producer Generated text: {generated_text!r}")
---
>         print(f"Generated text: {generated_text!r}")
76,77c75,78
<     sampling_params = SamplingParams(temperature=0, top_p=0.95, max_tokens=20)
<     # sampling_params = SamplingParams(temperature=0, max_tokens=100)
---
>     os.environ["CUDA_VISIBLE_DEVICES"] = "1"
> 
>     sampling_params = SamplingParams(temperature=0, top_p=0.95, max_tokens=10)
> 
79c80
<         '{"kv_connector":"LMCacheConnectorV1", "kv_role":"kv_consumer"}')
---
>         '{"kv_connector":"LMCacheConnectorV1", "kv_role":"kv_both"}')
82c83
<     llm = LLM(model=MODEL,
---
>     llm = LLM(model="mistralai/Mistral-7B-Instruct-v0.2",
86,87c87
<               tensor_parallel_size=2,
<               enforce_eager=False)
---
>               enforce_eager=True)
96c96
<         print(f"Consumer Generated text: {generated_text!r}")
---
>         print(f"Generated text: {generated_text!r}")
103,105c103,106
<     server_proc = subprocess.Popen(
<         ["python", "-m", "lmcache.v1.server", "localhost",
<          str(port)])
---
>     server_proc = subprocess.Popen([
>         "python", "-m", "lmcache.experimental.server", "localhost",
>         str(port)
>     ])
113d113
< 
115c115
<     print("libin kvshare store start")
---
> 
119d118
<     print("libin kvshare retrieve start")
122,124c121
<     print("libin kvshare retrieve done")
<     store_process.join()
<     retrieve_process.join()
---
> 
125a123
>     store_process.join()
