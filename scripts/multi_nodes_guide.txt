Steps for 2 nodes

1. environment setup
	a. *Identical* SW stack on 2 nodes - driver, docker, vllm version
	b. *Identical* model path on 2 nodes - i.e.  both nodes load weights from /mnt/disk0/model ....
	c. make sure 2 nodes are routed in same switch/router. their IPs are like: 192.168.1.100 & 192.168.1.200
	d. make sure hccl demo test can pass with the IPs. following is an example for head & node
		head: HCCL_COMM_ID=192.168.1.112:5555 python3 run_hccl_demo.py --test all_reduce --nranks 16 --loop 1000 --node_id 0 --size 32m --ranks_per_node 8
 		node: HCCL_COMM_ID=192.168.1.112:5555 python3 run_hccl_demo.py --test all_reduce --nranks 16 --loop 1000 --node_id 1 --size 32m --ranks_per_node 8
 	e. docker image must have envs
		--env PT_HPU_ENABLE_LAZY_COLLECTIVES=true
        	--env PT_HPUGRAPH_DISABLE_TENSOR_CACHE=1
		--env VLLM_HOST_IP=<your ip> # host & node must be different
		--env GLOO_SOCKET_IFNAME=<your network device>  # host & node might be same or different
	f. pip list | grep triton
		make sure trition is 3.1.0 version

2. pip install vllm ...
3. cd vllm/script, edit multi_nodes_source.sh
	a. adjust following envs per demand
		export VLLM_GPU_MEMORY_UTILIZATION=0.98
		export VLLM_GRAPH_RESERVED_MEM=0.35
		export VLLM_GRAPH_PROMPT_RATIO=0
	b. adjust following paras according to workloads
		max_num_batched_tokens=2048
		max_num_seqs=256
		input_min=1024
		input_max=1024
		output_max=1024
	c. both head & node
		source multi_nodes_source.sh 

4. start rays on head & node
	<head>: ray start --head --port=<port number, like 8850>
	<node>: ray start --address='IP:port'. i.e ray start --address='192.168.1.112:8850'

5. on head, start command line. !!! make sure the paras(mentioned in 4.b) in commnad line aligns to the settings in multi_nodes_source.sh" !!!

