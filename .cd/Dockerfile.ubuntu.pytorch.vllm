# Copyright (C) 2025 Habana Labs, Ltd. an Intel Company
# SPDX-License-Identifier: Apache-2.0

# Parameterize base image components
ARG DOCKER_URL=vault.habana.ai/gaudi-docker
ARG VERSION=1.21.1
ARG BASE_NAME=ubuntu22.04
ARG PT_VERSION=2.6.0
ARG REVISION=latest
ARG REPO_TYPE=habanalabs

FROM ${DOCKER_URL}/${VERSION}/${BASE_NAME}/${REPO_TYPE}/pytorch-installer-${PT_VERSION}:${REVISION}

# Parameterize commit/branch for vllm-fork checkout
ARG VLLM_FORK_COMMIT=v0.7.2+Gaudi-1.21.0

ENV OMPI_MCA_btl_vader_single_copy_mechanism=none

RUN apt update && \
    apt install -y gettext moreutils jq && \
    ln -sf /usr/bin/python3 /usr/bin/python
WORKDIR /root

ENV VLLM_PATH=/workspace/vllm

# Clone the vllm-fork repository
RUN mkdir -p $VLLM_PATH && \
    git clone https://github.com/HabanaAI/vllm-fork.git $VLLM_PATH && \
    cd $VLLM_PATH && \
    git remote add upstream https://github.com/vllm-project/vllm.git && \
    git fetch upstream --tags || true && \
    git checkout ${VLLM_FORK_COMMIT}

# Install vllm-fork inside the container
ENV VLLM_TARGET_DEVICE=hpu
RUN pip install -v -r $VLLM_PATH/requirements-hpu.txt
RUN pip install -v -e $VLLM_PATH --no-build-isolation
RUN pip install -v -e $VLLM_PATH/tests/vllm_test_utils --no-build-isolation

# Copy utility scripts and configuration
RUN mkdir -p /root/scripts/
COPY templates /root/scripts/templates/
COPY entrypoints /root/scripts/entrypoints/
COPY server /root/scripts/server/
COPY benchmark /root/scripts/benchmark/
WORKDIR /root/scripts

# Set entrypoint script
ENTRYPOINT ["python3", "-m", "entrypoints.entrypoint_main"]
