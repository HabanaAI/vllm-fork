services:
  vllm-server:
    image: docker_auto_24
    environment:
      - HF_HOME=/mnt/hf_cache
      - HF_TOKEN=${HF_TOKEN}
      - HABANA_VISIBLE_DEVICES=${HABANA_VISIBLE_DEVICES:-all}
      - PYTHONUNBUFFERED=1
    volumes:
      - /mnt/hf_cache:/mnt/hf_cache
    ports:
      - "8000:8000"
    cap_add:
      - SYS_NICE
    ipc: host
    runtime: habana
    restart: unless-stopped
    command: ["server", "--config-file", "${SERVER_CONFIG_FILE}", "--config-name", "${SERVER_CONFIG_NAME}"]
    healthcheck:
      test: ["CMD", "sh", "-c", "[ -f logs/vllm_server.log ] && grep -q 'Application startup complete' logs/vllm_server.log"]
      interval: 10s
      timeout: 2s
      retries: 500
      start_period: 10s

  benchmark:
    image: docker_auto_24
    network_mode: service:vllm-server
    depends_on:
      vllm-server:
        condition: service_healthy
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - PYTHONUNBUFFERED=1
    command: ["benchmark", "--config-file", "${BENCHMARK_CONFIG_FILE}", "--config-name", "${BENCHMARK_CONFIG_NAME}"]
