services:
  config-prep:
    image: python:3.11
    volumes:
      - .:/app
    working_dir: /app
    environment:
      - CONFIG_FILE
      - TEST_NAME
    entrypoint: ["sh", "-c", "pip install pyyaml && python docker_compose_prepare_env.py"]

  vllm-server:
    image: docker_auto_1
    depends_on:
      - config-prep
    env_file:
      - .env.generated
    environment:
      - HF_HOME=/mnt/hf_cache
      - HF_TOKEN=${HF_TOKEN}
      - HABANA_VISIBLE_DEVICES=${HABANA_VISIBLE_DEVICES:-all}
    volumes:
      - /mnt/hf_cache:/mnt/hf_cache
      - /software/users/tthaddey/:/software/users/tthaddey/
    ports:
      - "8000:8000"
    cap_add:
      - SYS_NICE
    ipc: host
    runtime: habana
    restart: unless-stopped
    command: []
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/metrics"]
      interval: 5s
      timeout: 5s
      retries: 500
      start_period: 5s

  benchmark:
    image: docker_auto_1
    network_mode: service:vllm-server
    depends_on:
      vllm-server:
        condition: service_healthy
    env_file:
      - .env.generated
    volumes:
      - .:/app
    working_dir: /app
    entrypoint: >
      sh -c "sleep 10 && HF_TOKEN=xxx python3 /workspace/vllm/benchmarks/benchmark_serving.py \
      --model meta-llama/Llama-3.1-8B-Instruct \
      --base-url http://localhost:8000 \
      --backend openai-chat \
      --endpoint /v1/chat/completions \
      --dataset-name hf \
      --dataset-path openai_human_eval \
      --hf-split train \
      --num-prompts 500 \
      --max-concurrency 64 \
      --metric-percentiles 90"
