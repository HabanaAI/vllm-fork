services:
  vllm-server:
    image: ${DOCKER_IMAGE}
    environment:
      - MODEL
      - HF_HOME=/mnt/hf_cache
      - HF_TOKEN
      - HABANA_VISIBLE_DEVICES=${HABANA_VISIBLE_DEVICES:-all}
      - PYTHONUNBUFFERED=1
      - PT_HPU_LAZY_MODE
      - VLLM_DECODE_BLOCK_BUCKET_STEP
      - VLLM_DECODE_BS_BUCKET_STEP
      - VLLM_PROMPT_BS_BUCKET_STEP
      - VLLM_PROMPT_SEQ_BUCKET_STEP
      - VLLM_SKIP_WARMUP
      - MAX_MODEL_LEN
      - MAX_NUM_SEQS
      - TENSOR_PARALLEL_SIZE
    volumes:
      - /mnt/hf_cache:/mnt/hf_cache
    ports:
      - "8000:8000"
    cap_add:
      - SYS_NICE
    ipc: host
    runtime: habana
    restart: unless-stopped
    command: ["server", "--config-file", "${VLLM_SERVER_CONFIG_FILE}", "--config-name", "${VLLM_SERVER_CONFIG_NAME}"]
    healthcheck:
      test: ["CMD", "sh", "-c", "[ -f logs/vllm_server.log ] && grep -q 'Application startup complete' logs/vllm_server.log"]
      interval: 10s
      timeout: 2s
      retries: 500
      start_period: 10s

  benchmark:
    profiles: ["benchmark"]
    image: ${DOCKER_IMAGE}
    network_mode: service:vllm-server
    depends_on:
      vllm-server:
        condition: service_healthy
    environment:
      - MODEL
      - HF_TOKEN=${HF_TOKEN}
      - PYTHONUNBUFFERED=1
      - INPUT_TOK
      - OUTPUT_TOK
      - CON_REQ
      - NUM_PROMPTS
    command: ["benchmark", "--config-file", "${VLLM_BENCHMARK_CONFIG_FILE}", "--config-name", "${VLLM_BENCHMARK_CONFIG_NAME}"]
